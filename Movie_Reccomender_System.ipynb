{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrPNW9ysWqEQe4gGcfbxMz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prishita17/Movie-Recommendation-System/blob/main/Movie_Reccomender_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, I will attempt at implementing a few recommendation algorithms (content based, popularity based and collaborative filtering) and try to build an ensemble of these models to come up with our final recommendation system. With us, we have two MovieLens datasets.\n",
        "\n",
        "1. The Full Dataset: Consists of 26,000,000 ratings and 750,000 tag applications applied to 45,000 movies by 270,000 users. Includes tag genome data with 12 million relevance scores across 1,100 tags.\n",
        "2. The Small Dataset: Comprises of 100,000 ratings and 1,300 tag applications applied to 9,000 movies by 700 users.\n",
        "\n",
        "I will build a Simple Recommender using movies from the Full Dataset whereas all personalised recommender systems will make use of the small dataset (due to the computing power I possess being very limited). As a first step, I will build my simple recommender system."
      ],
      "metadata": {
        "id": "43EJ-ehAQkur"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJeem2tTFB6c"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from ast import literal_eval\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from surprise import Reader, Dataset, SVD, evaluate\n",
        "\n",
        "import warnings; warnings.simplefilter('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Simple Recommender\n",
        "\n",
        "The simple recommender offers generalized recommnendations to every user based on movie popularity and (sometimes) genre. The basic idea behind this recommender is that movies that are more popular and more critically acclaimed will have a higher probability of being liked by the average audience. This model does not give personalized recommendations based on the user.\n",
        "\n",
        "\n",
        "The implementation of this model is extremely trivial. All we have to do is sort our movies based on ratings and popularity and display the top movies of our list. As an added step, we can pass in a genre argument to get the top movies of a particular genre.\n",
        "\n"
      ],
      "metadata": {
        "id": "kp_OJzfIRD1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0qFgO_4RIa4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84bcedbf",
        "outputId": "5112f569-0c93-4004-b543-af3be45bc9c5"
      },
      "source": [
        "!pip install surprise"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting surprise\n",
            "  Downloading surprise-0.1-py2.py3-none-any.whl.metadata (327 bytes)\n",
            "Collecting scikit-surprise (from surprise)\n",
            "  Downloading scikit_surprise-1.1.4.tar.gz (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-surprise->surprise) (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-surprise->surprise) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-surprise->surprise) (1.16.2)\n",
            "Downloading surprise-0.1-py2.py3-none-any.whl (1.8 kB)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.4-cp312-cp312-linux_x86_64.whl size=2611303 sha256=9e6363d5d2cb00e4ec91e6c8675e27b1b6b14e4123193883303075206c9c200d\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/fa/bc/739bc2cb1fbaab6061854e6cfbb81a0ae52c92a502a7fa454b\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise, surprise\n",
            "Successfully installed scikit-surprise-1.1.4 surprise-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl --request GET \\\n",
        "     --url https://api.themoviedb.org/3/authentication \\\n",
        "     --header 'accept: application/json'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1BvQRQVWXun",
        "outputId": "282b03da-b325-44d6-f69e-cc7597016e5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"status_code\":7,\"status_message\":\"Invalid API key: You must be granted a valid key.\",\"success\":false}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Replace '../input/movies_metadata.csv' with the actual path to your file\n",
        "md = pd.read_csv('../input/movies_metadata.csv')\n",
        "# md.read() # Removed as DataFrame objects do not have a read() method"
      ],
      "metadata": {
        "id": "4tNkud6TFm2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import literal_eval\n",
        "\n",
        "df['genres'] = df['genres'].fillna('[]').apply(literal_eval).apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])"
      ],
      "metadata": {
        "id": "ZX_OxRFnHWgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I use the TMDB Ratings to come up with our Top Movies Chart. I will use IMDB's weighted rating formula to construct my chart. Mathematically, it is represented as follows:\n",
        "\n",
        "Weighted Rating (WR) =  (vv+m.R)+(mv+m.C)\n",
        "\n",
        "\n",
        "where,\n",
        "\n",
        "v is the number of votes for the movie\n",
        "m is the minimum votes required to be listed in the chart\n",
        "R is the average rating of the movie\n",
        "C is the mean vote across the whole report\n",
        "The next step is to determine an appropriate value for m, the minimum votes required to be listed in the chart. We will use 95th percentile as our cutoff. In other words, for a movie to feature in the charts, it must have more votes than at least 95% of the movies in the list.\n",
        "\n",
        "I will build our overall Top 250 Chart and will define a function to build charts for a particular genre. Let's begin!"
      ],
      "metadata": {
        "id": "4CUQ6JllbFe3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vote_counts = md[md['vote_count'].notnull()]['vote_count'].astype('int')\n",
        "vote_averages = md[md['vote_average'].notnull()]['vote_average'].astype('int')\n",
        "C = vote_averages.mean()\n",
        "C"
      ],
      "metadata": {
        "id": "lMoQwfn2KazX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = vote_counts.quantile(0.95)\n",
        "m"
      ],
      "metadata": {
        "id": "OJZ5tmRxKqLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "md['year'] = pd.to_datetime(md['release_date'], errors='coerce').apply(lambda x: str(x).split('-')[0] if x != np.nan else np.nan)\n"
      ],
      "metadata": {
        "id": "DItckXJQKtea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qualified = md[(md['vote_count'] >= m) & (md['vote_count'].notnull()) & (md['vote_average'].notnull())][['title', 'year', 'vote_count', 'vote_average', 'popularity', 'genres']]\n",
        "qualified['vote_count'] = qualified['vote_count'].astype('int')\n",
        "qualified['vote_average'] = qualified['vote_average'].astype('int')\n",
        "qualified.shape"
      ],
      "metadata": {
        "id": "4sZPuBc2KuN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Therefore, to qualify to be considered for the chart, a movie has to have at least 434 votes on TMDB. We also see that the average rating for a movie on TMDB is 5.244 on a scale of 10. 2274 Movies qualify to be on our chart."
      ],
      "metadata": {
        "id": "0Ow1FqpGbVSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_rating(x):\n",
        "    v = x['vote_count']\n",
        "    R = x['vote_average']\n",
        "    return (v/(v+m) * R) + (m/(m+v) * C)"
      ],
      "metadata": {
        "id": "YEzRgmDIKzO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qualified['wr'] = qualified.apply(weighted_rating, axis=1)\n"
      ],
      "metadata": {
        "id": "j_Dsyjj1K3hI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qualified = qualified.sort_values('wr', ascending=False).head(250)\n"
      ],
      "metadata": {
        "id": "dvZ0DefpK6cX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qualified.head(15)"
      ],
      "metadata": {
        "id": "GVlncUOEK9fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that three Christopher Nolan Films, Inception, The Dark Knight and Interstellar occur at the very top of our chart. The chart also indicates a strong bias of TMDB Users towards particular genres and directors.\n",
        "\n",
        "Let us now construct our function that builds charts for particular genres. For this, we will use relax our default conditions to the 85th percentile instead of 95."
      ],
      "metadata": {
        "id": "VSZ4fVNcccBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s = md.apply(lambda x: pd.Series(x['genres']),axis=1).stack().reset_index(level=1, drop=True)\n",
        "s.name = 'genre'\n",
        "gen_md = md.drop('genres', axis=1).join(s)"
      ],
      "metadata": {
        "id": "AWRxtyGQLC14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_chart(genre, percentile=0.85):\n",
        "    df = gen_md[gen_md['genre'] == genre]\n",
        "    vote_counts = df[df['vote_count'].notnull()]['vote_count'].astype('int')\n",
        "    vote_averages = df[df['vote_average'].notnull()]['vote_average'].astype('int')\n",
        "    C = vote_averages.mean()\n",
        "    m = vote_counts.quantile(percentile)\n",
        "\n",
        "    qualified = df[(df['vote_count'] >= m) & (df['vote_count'].notnull()) & (df['vote_average'].notnull())][['title', 'year', 'vote_count', 'vote_average', 'popularity']]\n",
        "    qualified['vote_count'] = qualified['vote_count'].astype('int')\n",
        "    qualified['vote_average'] = qualified['vote_average'].astype('int')\n",
        "\n",
        "    qualified['wr'] = qualified.apply(lambda x: (x['vote_count']/(x['vote_count']+m) * x['vote_average']) + (m/(m+x['vote_count']) * C), axis=1)\n",
        "    qualified = qualified.sort_values('wr', ascending=False).head(250)\n",
        "\n",
        "    return qualified\n"
      ],
      "metadata": {
        "id": "20GvdSO2LDn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us see our method in action by displaying the Top 15 Romance Movies (Romance almost didn't feature at all in our Generic Top Chart despite being one of the most popular movie genres)."
      ],
      "metadata": {
        "id": "MUkyZLR9chlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "build_chart('Romance').head(15)"
      ],
      "metadata": {
        "id": "gay2aTcoLGvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Content Based Recommender\n",
        "The recommender we built in the previous section suffers some severe limitations. For one, it gives the same recommendation to everyone, regardless of the user's personal taste. If a person who loves romantic movies (and hates action) were to look at our Top 15 Chart, s/he wouldn't probably like most of the movies. If s/he were to go one step further and look at our charts by genre, s/he wouldn't still be getting the best recommendations.\n",
        "\n",
        "For instance, consider a person who loves Dilwale Dulhania Le Jayenge, My Name is Khan and Kabhi Khushi Kabhi Gham. One inference we can obtain is that the person loves the actor Shahrukh Khan and the director Karan Johar. Even if s/he were to access the romance chart, s/he wouldn't find these as the top recommendations.\n",
        "\n",
        "To personalise our recommendations more, I am going to build an engine that computes similarity between movies based on certain metrics and suggests movies that are most similar to a particular movie that a user liked. Since we will be using movie metadata (or content) to build this engine, this also known as Content Based Filtering.\n",
        "\n",
        "I will build two Content Based Recommenders based on:\n",
        "\n",
        "Movie Overviews and Taglines\n",
        "Movie Cast, Crew, Keywords and Genre\n",
        "Also, as mentioned in the introduction, I will be using a subset of all the movies available to us due to limiting computing power available to me."
      ],
      "metadata": {
        "id": "fe40WFKpcnq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "links_small = pd.read_csv('../input/links_small.csv')\n",
        "links_small = links_small[links_small['tmdbId'].notnull()]['tmdbId'].astype('int')\n"
      ],
      "metadata": {
        "id": "cMNiU3TGLJxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "md = md.drop([19730, 29503, 35587])\n"
      ],
      "metadata": {
        "id": "SpGgU760LNKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "md['id'] = md['id'].astype('int')"
      ],
      "metadata": {
        "id": "1pVLw6k6LQC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smd = md[md['id'].isin(links_small)]\n",
        "smd.shape"
      ],
      "metadata": {
        "id": "kufvayduLS-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have 9099 movies avaiable in our small movies metadata dataset which is 5 times smaller than our original dataset of 45000 movies.\n",
        "\n",
        "Movie Description Based Recommender\n",
        "Let us first try to build a recommender using movie descriptions and taglines. We do not have a quantitative metric to judge our machine's performance so this will have to be done qualitatively."
      ],
      "metadata": {
        "id": "lVLJ-phjcul-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "smd['tagline'] = smd['tagline'].fillna('')\n",
        "smd['description'] = smd['overview'] + smd['tagline']\n",
        "smd['description'] = smd['description'].fillna('')"
      ],
      "metadata": {
        "id": "e3mYpkYMLY-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')\n",
        "tfidf_matrix = tf.fit_transform(smd['description'])"
      ],
      "metadata": {
        "id": "0MvLTtRrLZ1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_matrix.shape"
      ],
      "metadata": {
        "id": "BFYwZjbkLc2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cosine Similarity\n",
        "I will be using the Cosine Similarity to calculate a numeric quantity that denotes the similarity between two movies. Mathematically, it is defined as follows:\n",
        "\n",
        "cosine(x,y)=x.y⊺||x||.||y||\n",
        "\n",
        "\n",
        "Since we have used the TF-IDF Vectorizer, calculating the Dot Product will directly give us the Cosine Similarity Score. Therefore, we will use sklearn's linear_kernel instead of cosine_similarities since it is much faster."
      ],
      "metadata": {
        "id": "kpOMhjKydF4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)"
      ],
      "metadata": {
        "id": "teBl0joGLgAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_sim[0]"
      ],
      "metadata": {
        "id": "aCGJMWqxLjT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have a pairwise cosine similarity matrix for all the movies in our dataset. The next step is to write a function that returns the 30 most similar movies based on the cosine similarity score."
      ],
      "metadata": {
        "id": "rmZ0uOdMdOc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "smd = smd.reset_index()\n",
        "titles = smd['title']\n",
        "indices = pd.Series(smd.index, index=smd['title'])"
      ],
      "metadata": {
        "id": "1GPDGFW2Lmwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_recommendations(title):\n",
        "    idx = indices[title]\n",
        "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "    sim_scores = sim_scores[1:31]\n",
        "    movie_indices = [i[0] for i in sim_scores]\n",
        "    return titles.iloc[movie_indices]\n"
      ],
      "metadata": {
        "id": "ohwb17i1Lpbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're all set. Let us now try and get the top recommendations for a few movies and see how good the recommendations are.\n",
        "\n"
      ],
      "metadata": {
        "id": "lL7C1RKCdVvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_recommendations('The Godfather').head(10)\n"
      ],
      "metadata": {
        "id": "cWC1eiqnLsqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_recommendations('The Dark Knight').head(10)"
      ],
      "metadata": {
        "id": "DNM8F4gtLvlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that for The Dark Knight, our system is able to identify it as a Batman film and subsequently recommend other Batman films as its top recommendations. But unfortunately, that is all this system can do at the moment. This is not of much use to most people as it doesn't take into considerations very important features such as cast, crew, director and genre, which determine the rating and the popularity of a movie. Someone who liked The Dark Knight probably likes it more because of Nolan and would hate Batman Forever and every other substandard movie in the Batman Franchise.\n",
        "\n",
        "Therefore, we are going to use much more suggestive metadata than Overview and Tagline. In the next subsection, we will build a more sophisticated recommender that takes genre, keywords, cast and crew into consideration.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Metadata Based Recommender\n",
        "\n",
        "To build our standard metadata based content recommender, we will need to merge our current dataset with the crew and the keyword datasets. Let us prepare this data as our first step.\n",
        "\n"
      ],
      "metadata": {
        "id": "oznUlxmteCjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "credits = pd.read_csv('../input/credits.csv')\n",
        "keywords = pd.read_csv('../input/keywords.csv')"
      ],
      "metadata": {
        "id": "hSMlUXgQLyYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keywords['id'] = keywords['id'].astype('int')\n",
        "credits['id'] = credits['id'].astype('int')\n",
        "md['id'] = md['id'].astype('int')"
      ],
      "metadata": {
        "id": "kJfNOnKnL2ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "md.shape"
      ],
      "metadata": {
        "id": "7cHSjXJ6QZZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "md = md.merge(credits, on='id')\n",
        "md = md.merge(keywords, on='id')"
      ],
      "metadata": {
        "id": "hxE7s-uDQe3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have our cast, crew, genres and credits, all in one dataframe. Let us wrangle this a little more using the following intuitions:\n",
        "\n",
        "1. Crew: From the crew, we will only pick the director as our feature since the others don't contribute that much to the feel of the movie.\n",
        "2. Cast: Choosing Cast is a little more tricky. Lesser known actors and minor roles do not really affect people's opinion of a movie. Therefore, we must only select the major characters and their respective actors. Arbitrarily we will choose the top 3 actors that appear in the credits list."
      ],
      "metadata": {
        "id": "nlZ8yXtWeVnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "smd = md[md['id'].isin(links_small)]\n",
        "smd.shape\n"
      ],
      "metadata": {
        "id": "QfToTPVwQff5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smd['cast'] = smd['cast'].apply(literal_eval)\n",
        "smd['crew'] = smd['crew'].apply(literal_eval)\n",
        "smd['keywords'] = smd['keywords'].apply(literal_eval)\n",
        "smd['cast_size'] = smd['cast'].apply(lambda x: len(x))\n",
        "smd['crew_size'] = smd['crew'].apply(lambda x: len(x))"
      ],
      "metadata": {
        "id": "A58uhj5SQimo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_director(x):\n",
        "    for i in x:\n",
        "        if i['job'] == 'Director':\n",
        "            return i['name']\n",
        "    return np.nan\n"
      ],
      "metadata": {
        "id": "AOZ7xX2QQnji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smd['director'] = smd['crew'].apply(get_director)"
      ],
      "metadata": {
        "id": "TGFhSsarQrWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smd['cast'] = smd['cast'].apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])\n",
        "smd['cast'] = smd['cast'].apply(lambda x: x[:3] if len(x) >=3 else x)\n"
      ],
      "metadata": {
        "id": "fj9wkSFPQwq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "My approach to building the recommender is going to be extremely hacky. What I plan on doing is creating a metadata dump for every movie which consists of genres, director, main actors and keywords. I then use a Count Vectorizer to create our count matrix as we did in the Description Recommender. The remaining steps are similar to what we did earlier: we calculate the cosine similarities and return movies that are most similar.\n",
        "\n",
        "These are steps I follow in the preparation of my genres and credits data:\n",
        "\n",
        "1. Strip Spaces and Convert to Lowercase from all our features. This way, our engine will not confuse between Johnny Depp and Johnny Galecki.\n",
        "2. Mention Director 3 times to give it more weight relative to the entire cast.\n"
      ],
      "metadata": {
        "id": "mKX_oON0ei87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "smd['keywords'] = smd['keywords'].apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])"
      ],
      "metadata": {
        "id": "aSeeMliJQzmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smd['cast'] = smd['cast'].apply(lambda x: [str.lower(i.replace(\" \", \"\")) for i in x])"
      ],
      "metadata": {
        "id": "mofMhFKwQ4Q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smd['director'] = smd['director'].astype('str').apply(lambda x: str.lower(x.replace(\" \", \"\")))\n",
        "smd['director'] = smd['director'].apply(lambda x: [x,x, x])"
      ],
      "metadata": {
        "id": "yewjUyQxQ8F2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keywords\n",
        "\n",
        "We will do a small amount of pre-processing of our keywords before putting them to any use. As a first step, we calculate the frequenct counts of every keyword that appears in the dataset."
      ],
      "metadata": {
        "id": "7UgWczPYewYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s = smd.apply(lambda x: pd.Series(x['keywords']),axis=1).stack().reset_index(level=1, drop=True)\n",
        "s.name = 'keyword'\n"
      ],
      "metadata": {
        "id": "ci_Hdjf4Q_Wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = s.value_counts()\n",
        "s[:5]"
      ],
      "metadata": {
        "id": "wLisfPWNRCfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keywords occur in frequencies ranging from 1 to 610. We do not have any use for keywords that occur only once. Therefore, these can be safely removed. Finally, we will convert every word to its stem so that words such as Dogs and Dog are considered the same."
      ],
      "metadata": {
        "id": "p-Ajuoy1e6YO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s = s[s > 1]"
      ],
      "metadata": {
        "id": "DN5loDBRRG-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = SnowballStemmer('english')\n",
        "stemmer.stem('dogs')"
      ],
      "metadata": {
        "id": "N7GP5YR8RL-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_keywords(x):\n",
        "    words = []\n",
        "    for i in x:\n",
        "        if i in s:\n",
        "            words.append(i)\n",
        "    return words\n"
      ],
      "metadata": {
        "id": "mg2GOu4TRPei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smd['keywords'] = smd['keywords'].apply(filter_keywords)\n",
        "smd['keywords'] = smd['keywords'].apply(lambda x: [stemmer.stem(i) for i in x])\n",
        "smd['keywords'] = smd['keywords'].apply(lambda x: [str.lower(i.replace(\" \", \"\")) for i in x])\n"
      ],
      "metadata": {
        "id": "JiawhVM3RTss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smd['soup'] = smd['keywords'] + smd['cast'] + smd['director'] + smd['genres']\n",
        "smd['soup'] = smd['soup'].apply(lambda x: ' '.join(x))"
      ],
      "metadata": {
        "id": "Fn10JQfGRXp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = CountVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')\n",
        "count_matrix = count.fit_transform(smd['soup'])"
      ],
      "metadata": {
        "id": "5GPyhwfWRbhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_sim = cosine_similarity(count_matrix, count_matrix)\n"
      ],
      "metadata": {
        "id": "N-AdELhDRfpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smd = smd.reset_index()\n",
        "titles = smd['title']\n",
        "indices = pd.Series(smd.index, index=smd['title'])"
      ],
      "metadata": {
        "id": "m2UH-MuRRkYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will reuse the get_recommendations function that we had written earlier. Since our cosine similarity scores have changed, we expect it to give us different (and probably better) results. Let us check for The Dark Knight again and see what recommendations I get this time around.\n",
        "\n",
        "\n",
        "Let me also get recommendations for another movie, Mean Girls which happens to be my girlfriend's favorite movie."
      ],
      "metadata": {
        "id": "eG2Iq5D6fF3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_recommendations('The Dark Knight').head(10)"
      ],
      "metadata": {
        "id": "LA_sSxJkRo0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_recommendations('Mean Girls').head(10)\n"
      ],
      "metadata": {
        "id": "vo1X44-oRr7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Popularity and Ratings\n",
        "\n",
        "One thing that we notice about our recommendation system is that it recommends movies regardless of ratings and popularity. It is true that Batman and Robin has a lot of similar characters as compared to The Dark Knight but it was a terrible movie that shouldn't be recommended to anyone.\n",
        "\n",
        "\n",
        "Therefore, we will add a mechanism to remove bad movies and return movies which are popular and have had a good critical response.\n",
        "\n",
        "\n",
        "I will take the top 25 movies based on similarity scores and calculate the vote of the 60th percentile movie. Then, using this as the value of  m\n",
        " , we will calculate the weighted rating of each movie using IMDB's formula like we did in the Simple Recommender section."
      ],
      "metadata": {
        "id": "gDGB6wtINfu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def improved_recommendations(title):\n",
        "    idx = indices[title]\n",
        "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "    sim_scores = sim_scores[1:26]\n",
        "    movie_indices = [i[0] for i in sim_scores]\n",
        "\n",
        "    movies = smd.iloc[movie_indices][['title', 'vote_count', 'vote_average', 'year']]\n",
        "    vote_counts = movies[movies['vote_count'].notnull()]['vote_count'].astype('int')\n",
        "    vote_averages = movies[movies['vote_average'].notnull()]['vote_average'].astype('int')\n",
        "    C = vote_averages.mean()\n",
        "    m = vote_counts.quantile(0.60)\n",
        "    qualified = movies[(movies['vote_count'] >= m) & (movies['vote_count'].notnull()) & (movies['vote_average'].notnull())]\n",
        "    qualified['vote_count'] = qualified['vote_count'].astype('int')\n",
        "    qualified['vote_average'] = qualified['vote_average'].astype('int')\n",
        "    qualified['wr'] = qualified.apply(weighted_rating, axis=1)\n",
        "    qualified = qualified.sort_values('wr', ascending=False).head(10)\n",
        "    return qualified"
      ],
      "metadata": {
        "id": "hh2bxpV1RvLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "improved_recommendations('The Dark Knight')"
      ],
      "metadata": {
        "id": "JFyRX_1xR0jZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let me also get the recommendations for Mean Girls\n",
        "\n"
      ],
      "metadata": {
        "id": "6S8exiUZODgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "improved_recommendations('Mean Girls')"
      ],
      "metadata": {
        "id": "WgzbDXLFR4hL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unfortunately, Batman and Robin does not disappear from our recommendation list. This is probably due to the fact that it is rated a 4, which is only slightly below average on TMDB. It certainly doesn't deserve a 4 when amazing movies like The Dark Knight Rises has only a 7. However, there is nothing much we can do about this. Therefore, we will conclude our Content Based Recommender section here and come back to it when we build a hybrid engine.\n",
        "\n"
      ],
      "metadata": {
        "id": "_HUhQh__OMb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collaborative Filtering\n",
        "\n",
        "Our content based engine suffers from some severe limitations. It is only capable of suggesting movies which are close to a certain movie. That is, it is not capable of capturing tastes and providing recommendations across genres.\n",
        "\n",
        "\n",
        "Also, the engine that we built is not really personal in that it doesn't capture the personal tastes and biases of a user. Anyone querying our engine for recommendations based on a movie will receive the same recommendations for that movie, regardless of who s/he is.\n",
        "\n",
        "\n",
        "Therefore, in this section, we will use a technique called Collaborative Filtering to make recommendations to Movie Watchers. Collaborative Filtering is based on the idea that users similar to a me can be used to predict how much I will like a particular product or service those users have used/experienced but I have not.\n",
        "\n",
        "\n",
        "I will not be implementing Collaborative Filtering from scratch. Instead, I will use the Surprise library that used extremely powerful algorithms like Singular Value Decomposition (SVD) to minimise RMSE (Root Mean Square Error) and give great recommendations.\n",
        "\n"
      ],
      "metadata": {
        "id": "81jlFgiqOUH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reader = Reader()"
      ],
      "metadata": {
        "id": "-mk6RJ_ER8CM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings = pd.read_csv('../input/ratings_small.csv')\n",
        "ratings.head()"
      ],
      "metadata": {
        "id": "uXrXm27SSAq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n",
        "data.split(n_folds=5)\n"
      ],
      "metadata": {
        "id": "scrVvhQuSEBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svd = SVD()\n",
        "evaluate(svd, data, measures=['RMSE', 'MAE'])"
      ],
      "metadata": {
        "id": "ztLzkcFgSH3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We get a mean Root Mean Sqaure Error of 0.8963 which is more than good enough for our case. Let us now train on our dataset and arrive at predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "HhFf0aK8OfUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = data.build_full_trainset()\n",
        "svd.train(trainset)"
      ],
      "metadata": {
        "id": "j4Z05eBzSL2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us pick user 5000 and check the ratings s/he has given."
      ],
      "metadata": {
        "id": "KX6k3fXiOlaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ratings[ratings['userId'] == 1]"
      ],
      "metadata": {
        "id": "IV1sLXrgSPh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svd.predict(1, 302, 3)"
      ],
      "metadata": {
        "id": "Zfmg0GXESTNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hybrid Recommender\n",
        "\n",
        "In this section, I will try to build a simple hybrid recommender that brings together techniques we have implemented in the content based and collaborative filter based engines. This is how it will work:\n",
        "\n",
        "\n",
        "1. Input: User ID and the Title of a Movie\n",
        "2. Output: Similar movies sorted on the basis of expected ratings by that particular user.\n"
      ],
      "metadata": {
        "id": "ugBBaX-gO1Eg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_int(x):\n",
        "    try:\n",
        "        return int(x)\n",
        "    except:\n",
        "        return np.nan"
      ],
      "metadata": {
        "id": "GIuw46NWSW3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id_map = pd.read_csv('../input/links_small.csv')[['movieId', 'tmdbId']]\n",
        "id_map['tmdbId'] = id_map['tmdbId'].apply(convert_int)\n",
        "id_map.columns = ['movieId', 'id']\n",
        "id_map = id_map.merge(smd[['title', 'id']], on='id').set_index('title')\n",
        "#id_map = id_map.set_index('tmdbId')"
      ],
      "metadata": {
        "id": "x54s12CvSa6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices_map = id_map.set_index('id')"
      ],
      "metadata": {
        "id": "0sEPVmu8SfPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hybrid(userId, title):\n",
        "    idx = indices[title]\n",
        "    tmdbId = id_map.loc[title]['id']\n",
        "    #print(idx)\n",
        "    movie_id = id_map.loc[title]['movieId']\n",
        "\n",
        "    sim_scores = list(enumerate(cosine_sim[int(idx)]))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "    sim_scores = sim_scores[1:26]\n",
        "    movie_indices = [i[0] for i in sim_scores]\n",
        "\n",
        "    movies = smd.iloc[movie_indices][['title', 'vote_count', 'vote_average', 'year', 'id']]\n",
        "    movies['est'] = movies['id'].apply(lambda x: svd.predict(userId, indices_map.loc[x]['movieId']).est)\n",
        "    movies = movies.sort_values('est', ascending=False)\n",
        "    return movies.head(10)"
      ],
      "metadata": {
        "id": "nSkKk6aESipo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hybrid(1, 'Avatar')"
      ],
      "metadata": {
        "id": "oKEOn_HYSnWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hybrid(500, 'Avatar')"
      ],
      "metadata": {
        "id": "YS5WIrMjSq0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that for our hybrid recommender, we get different recommendations for different users although the movie is the same. Hence, our recommendations are more personalized and tailored towards particular users."
      ],
      "metadata": {
        "id": "azB1V8w6PGtq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion\n",
        "In this notebook, I have built 4 different recommendation engines based on different ideas and algorithms. They are as follows:\n",
        "\n",
        "1. Simple Recommender: This system used overall TMDB Vote Count and Vote Averages to build Top Movies Charts, in general and for a specific genre. The IMDB Weighted Rating System was used to calculate ratings on which the sorting was finally performed.\n",
        "2. Content Based Recommender: We built two content based engines; one that took movie overview and taglines as input and the other which took metadata such as cast, crew, genre and keywords to come up with predictions. We also deviced a simple filter to give greater preference to movies with more votes and higher ratings.\n",
        "3. Collaborative Filtering: We used the powerful Surprise Library to build a collaborative filter based on single value decomposition. The RMSE obtained was less than 1 and the engine gave estimated ratings for a given user and movie.\n",
        "4. Hybrid Engine: We brought together ideas from content and collaborative filterting to build an engine that gave movie suggestions to a particular user based on the estimated ratings that it had internally calculated for that user."
      ],
      "metadata": {
        "id": "1cKyDBYePNMz"
      }
    }
  ]
}